from create_task import *

import numpy as np
import copy

import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torch.optim as optim
import torch.nn.functional as F
import torch.utils.data.sampler as sampler

from auxiliary_branch import *
from learner import learner
from create_dataset import TaskData
from LightmanNet import LightmanNet


trans_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),

])
trans_test = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.2, 0.2, 0.2)),

])


task_train_set = TaskData(root='dataset', train=True, transform=trans_train, download=False)
task_test_set = TaskData(root='dataset', train=False, transform=trans_test, download=False)

psi = [5]*20
batch_size = 100
kwargs = {'num_workers': 1, 'pin_memory': True}
dataset_train_loader = torch.utils.data.DataLoader(
    dataset=task_train_set,
    batch_size=batch_size,
    shuffle=True)

dataset_test_loader = torch.utils.data.DataLoader(
    dataset=task_test_set,
    batch_size=batch_size,
    shuffle=True)


device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
LabelGenerator = auxiliaryBranch(psi=psi).to(device)
gen_optimizer = optim.SGD(LabelGenerator.parameters(), lr=1e-3, weight_decay=5e-4)
gen_scheduler = optim.lr_scheduler.StepLR(gen_optimizer, step_size=50, gamma=0.5)


total_epoch = 200
train_batch = len(dataset_train_loader)
test_batch = len(dataset_test_loader)

model = learner(psi=psi).to(device)
optimizer = optim.SGD(model.parameters(), lr=0.01)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)


maxl = LightmanNet(multi_task_net=model, label_generator=LabelGenerator)

avg_cost = np.zeros([total_epoch, 5], dtype=np.float32)
k = 0
for index in range(total_epoch):
    cost = np.zeros(4, dtype=np.float32)

    # evaluate training data (training-step, update on theta_1)
    model.train()
    cifar100_train_dataset = iter(dataset_train_loader)
    for i in range(train_batch):
        train_data, train_label = cifar100_train_dataset.next()
        train_label = train_label.type(torch.LongTensor)
        train_data, train_label = train_data.to(device), train_label.to(device)

        train_pred1, train_pred2 = model(train_data)
        train_pred3 = LabelGenerator(train_data, train_label[:, 2])  # generate auxiliary labels

        # reset optimizers with zero gradient
        optimizer.zero_grad()
        gen_optimizer.zero_grad()

        # choose level 2/3 hierarchy, 20-class (gt) / 100-class classification (generated by labelgeneartor)
        train_loss1 = model.model_fit(train_pred1, train_label[:, 2], pri=True, num_output=20)
        train_loss2 = model.model_fit(train_pred2, train_pred3, pri=False, num_output=100)

        # compute cosine similarity between gradients from primary and auxiliary loss
        grads1 = torch.autograd.grad(torch.mean(train_loss1), model.parameters(), retain_graph=True, allow_unused=True)
        grads2 = torch.autograd.grad(torch.mean(train_loss2), model.parameters(), retain_graph=True, allow_unused=True)
        cos_mean = 0
        for l in range(len(grads1) - 8):  # only compute on shared representation (ignore task-specific fc-layers)
            grads1_ = grads1[l].view(grads1[l].shape[0], -1)
            grads2_ = grads2[l].view(grads2[l].shape[0], -1)
            cos_mean += torch.mean(F.cosine_similarity(grads1_, grads2_, dim=-1)) / (len(grads1) - 8)
        # cosine similarity evaluation ends here

        train_loss = torch.mean(train_loss1) + torch.mean(train_loss2)
        train_loss.backward()

        optimizer.step()

        train_predict_label1 = train_pred1.data.max(1)[1]
        train_acc1 = train_predict_label1.eq(train_label[:, 2]).sum().item() / batch_size

        cost[0] = torch.mean(train_loss1).item()
        cost[1] = train_acc1
        cost[2] = cos_mean
        k = k + 1
        avg_cost[index][0:3] += cost[0:3] / train_batch

    cifar100_train_dataset = iter(dataset_train_loader)
    for i in range(train_batch):
        train_data, train_label = cifar100_train_dataset.next()
        train_label = train_label.type(torch.LongTensor)
        train_data, train_label = train_data.to(device), train_label.to(device)

        # reset optimizer with zero gradient
        optimizer.zero_grad()
        gen_optimizer.zero_grad()

        maxl.unrolled_backward(train_data, train_label[:, 2], scheduler.get_last_lr()[0], optimizer)
        gen_optimizer.step()

    model.eval()
    with torch.no_grad():
        cifar100_test_dataset = iter(dataset_test_loader)
        for i in range(test_batch):
            test_data, test_label = cifar100_test_dataset.next()
            test_label = test_label.type(torch.LongTensor)
            test_data, test_label = test_data.to(device), test_label.to(device)

            test_pred1, test_pred2 = model(test_data)
            test_loss1 = model.model_fit(test_pred1, test_label[:, 2], pri=True, num_output=20)

            test_predict_label1 = test_pred1.data.max(1)[1]
            test_acc1 = test_predict_label1.eq(test_label[:, 2]).sum().item() / batch_size

            cost[0] = torch.mean(test_loss1).item()
            cost[1] = test_acc1

            avg_cost[index][3:] += cost[0:2] / test_batch

    scheduler.step()
    gen_scheduler.step()
    print('EPOCH: {:04d} Iter {:04d} | TRAIN [LOSS|ACC.]: PRI {:.4f} {:.4f} COSSIM {:.4f} || TEST: {:.4f} {:.4f}'
          .format(index, k, avg_cost[index][0], avg_cost[index][1], avg_cost[index][2], avg_cost[index][3],
                  avg_cost[index][4]))
